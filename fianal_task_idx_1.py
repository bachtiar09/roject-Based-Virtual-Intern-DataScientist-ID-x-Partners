# -*- coding: utf-8 -*-
"""fianal_task_idx_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lAKSJAsvhHBBDlQ5Kng1MEVCsQj2IVPi
"""

# library
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

url="/content/drive/MyDrive/loan_data_2007_2014.csv"

# Set the option to display all columns
pd.set_option('display.max_columns', None)
df = pd.read_csv(url)
df.head(3)

df.info()

df.shape

"""kareana terlalu banyak kolom dan mising value alangkah lebih baik untuk menghapus kolom dengan banyak mising value"""

# Hitung persentase missing value per kolom
missing_percentage = df.isnull().sum() / len(df)
# Dapatkan kolom yang memiliki lebih dari 50% missing value
cols_to_drop = missing_percentage[missing_percentage > 0.5].index
# Hapus kolom tersebut
df_1 = df.drop(cols_to_drop, axis=1)

print('before drop =',df.shape)
print('after  drop =',df_1.shape)

"""#EDA"""

df_1.sample(4)

"""##Statistical analysis"""

# memisahkan data berdasrkan tipe
cat = []
num = []
dat = []
for i in df_1.columns:
    if df_1[i].dtype == 'O':
        cat.append(i)
    elif df_1[i].dtype == 'int64' or df[i].dtype == 'float64':
        num.append(i)
    else:
        dat.append(i)

df_1[num].describe().transpose()

df_1[cat].describe().transpose()

cat_1=["term","grade","sub_grade","emp_length","home_ownership","verification_status","loan_status","pymnt_plan","purpose"
"purpose","initial_list_status","application_type"]

""""

##Univariate analysis
"""

plt.figure(figsize = (10,5))
for i in range(0, len(num)):
    plt.subplot(5,7, i+1)
    sns.boxplot(data= df_1, y=num[i], orient='v')
    plt.tight_layout()

df_1[num].shape

# Membagi list menjadi tiga
n = len(num) // 3

# Menyimpan tiga bagian dalam variabel num_a, num_b, dan num_c
num_a = num[:n]
num_b = num[n:2*n]
num_c = num[2*n:]

print("num_a:", num_a)
print("num_b:", num_b)
print("num_c:", num_c)

plt.figure(figsize = (10,5))
for i in range(0, len(num_a)):
    plt.subplot(4,4, i+1)
    sns.kdeplot(data= df_1, x=num_a[i])
    plt.tight_layout()

plt.figure(figsize = (10,5))
for i in range(0, len(num_b)):
    plt.subplot(4,5, i+1)
    sns.kdeplot(data= df_1, x=num_b[i])
    plt.tight_layout()

plt.figure(figsize = (10,5))
for i in range(0, len(num_b)):
    plt.subplot(4,5, i+1)
    sns.kdeplot(data= df_1, x=num_c[i])
    plt.tight_layout()

plt.figure(figsize = (30,20)) # Meningkatkan ukuran gambar
for i in range(0, len(cat_1)):
    plt.subplot(4,3, i+1)
    if cat_1[i] in df_1.columns: # Memeriksa apakah kolom ada dalam df_1
        sns.countplot(data= df_1, x=cat_1[i])
        plt.xticks(rotation=90)
    else:
        print(f"Kolom {cat_1[i]} tidak ada dalam DataFrame")
plt.tight_layout()



"""##Multivariate analysis"""

plt.figure(figsize=(22, 22))
sns.heatmap(df_1.corr(), cmap='Blues', annot=True, fmt='.2f')

"""#insight"""

# Select the features
features = [
    "annual_inc",
    "dti",
    "total_acc",
    "delinq_2yrs",
    "acc_now_delinq",
    "purpose",
]

df_1[features].sample(10)

df_1[features].isna().sum()

pur=df_1.groupby('purpose').agg({'id':'count'}).reset_index()
pur.columns=['purpose','total']
pur['%'] = round(pur.total*100/sum(pur.total),2)
pur.sort_values(by='%', ascending=False)

# Plot
plt.figure(figsize=(10,6))
barplot = sns.barplot(x='purpose', y='%', data=pur)
plt.title('Tujuan Pengajuan Pinjaman', fontsize=15, fontweight='bold')
plt.xticks(rotation=90)

# Add numbers on top of each bar
for p in barplot.patches:
    barplot.annotate(format(p.get_height(), '.2f'),
                     (p.get_x() + p.get_width() / 2., p.get_height()),
                     ha = 'center', va = 'center',
                     xytext = (0, 10),
                     textcoords = 'offset points')

plt.show()

df_1['delinq_2yrs'].value_counts()

dell= df_1.groupby('delinq_2yrs').agg({'id':'count'}).reset_index()
dell.columns=['delinq_2yrs','total']
dell

fig, ax = plt.subplots(figsize=(9, 7))
sns.barplot(y='total', x='delinq_2yrs', data=dell)
plt.title(' Jumlah Insiden Keterlambatan Pembayaran', fontsize='14', color='Blue')
plt.xlabel('Jumlah Keterlambatan', fontsize='12')
plt.ylabel('Insiden Keterlambatan', fontsize='12')
plt.bar_label(ax.containers[0], padding=2)
plt.tight_layout()

df_1.groupby('purpose').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

term = df_1.groupby('term').agg({'id':'count'}).reset_index()
term.columns=['term','total']
term['%'] = round(term.total*100/sum(term.total),3)
term

fig, ax = plt.subplots(figsize=(9, 7))
sns.barplot(x='term', y='%', data=term)
plt.title('Presentase Termin Pengajuan Pinjaman', fontsize='14', color='Blue')
plt.xlabel('Termin', fontsize='12')
plt.ylabel('Presentase', fontsize='12')
plt.bar_label(ax.containers[0], padding=2 , fontsize='14')
plt.tight_layout()

"""#data prepration"""

df_1['loan_status'].unique()

def label_loan_status(status):
    good_debt = ["Current", "Fully Paid", "Does not meet the credit policy. Status:Fully Paid"]
    if status in good_debt:
        return 1
    else:
        return 0

# Asumsikan bahwa 'df' adalah DataFrame Anda dan 'loan_status' adalah kolom yang berisi status pinjaman
df_1['target'] = df_1['loan_status'].apply(label_loan_status)

df_1[['loan_status','target']]

"""## fiture selection

Kemampuan membayar:
* Pendapatan: annual_inc
* Rasio utang terhadap pendapatan (debt-to-income ratio): dti

Riwayat kredit:
* Jumlah pinjaman yang pernah diambil: total_acc
* Terlambat bayar: delinq_2yrs
* Gagal bayar: acc_now_delinq
* lama pengajuan : term

Tujuan pinjaman: purpose

Kolom-kolom ini dapat memberikan informasi yang penting tentang kemampuan peminjam untuk membayar pinjaman, riwayat kredit peminjam, dan tujuan pinjaman peminjam.
"""

# Select the features
featur = [
    "annual_inc",
    "dti",
    "total_acc",
    "delinq_2yrs",
    "acc_now_delinq",
    "purpose",
    'term',
    'target'
]

df_select = df_1[featur]
df_select

"""## data cleansing

## mising value
"""

df_select.isna().sum()

df_select = df_select.dropna()
df_select.isna().sum()

"""## data duplicate"""

# cek duplikat
df_select.duplicated().sum()

# menghapus duplikat
df_select.drop_duplicates(inplace=True)
df_select.duplicated().sum()

"""##Outliers"""

from scipy import stats
print(f'Jumlah baris sebelum memfilter outlier: {len(df_select)}')

filtered_entries = np.array([True] * len(df_select))

for col in ['annual_inc','dti','total_acc','delinq_2yrs','acc_now_delinq']:

      Q1 = df[col].quantile(0.25)
      Q3 = df[col].quantile(0.75)
      IQR = Q3 - Q1
      low_limit = Q1 - (IQR * 1.5)
      high_limit = Q3 + (IQR * 1.5)

      filtered_entries = ((df_select[col] >= low_limit) & (df_select[col] <= high_limit)) & filtered_entries

df_select = df_select[filtered_entries]

print(f'Jumlah baris setelah memfilter outlier: {len(df_select)}')

df_select.shape

"""## encoding"""

from sklearn.preprocessing import LabelEncoder
# membuat instance dari LabelEncoder
le = LabelEncoder()

# melakukan label encoding pada kolom 'purpose'
df_select['purpose'] = le.fit_transform(df_select['purpose'])

# Menghapus karakter non-angka dan mengubah tipe data menjadi integer
df_select['term'] = df_select['term'].apply(lambda x: int(''.join(filter(str.isdigit, x))))

"""## Transformation"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
import matplotlib.pyplot as plt
import seaborn as sns
import time

# Split data
X = df_select.drop('target', axis=1)
y = df_select['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Apply SMOTE
from imblearn.over_sampling import SMOTE
sm = SMOTE(random_state=42)
X_train_res, y_train_res = sm.fit_resample(X_train, y_train)

# Normalisasi
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
X_train_res =  scaler.transform(X_train_res)

"""#modeling untuk beberapa model ml"""

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from time import time
import pandas as pd

def fit_ml_models(X_train, y_train, X_test, y_test):
    models = [
        ('Logistic Regression', LogisticRegression()),
        ('Decision Tree', DecisionTreeClassifier()),
        ('Random Forest', RandomForestClassifier()),
        ('Gradient Boosting', GradientBoostingClassifier()),
        ('KNN', KNeighborsClassifier())
    ]

    results = []

    for name, model in models:
        start = time()
        model.fit(X_train, y_train)
        end = time()
        time_taken = end - start

        y_pred = model.predict(X_test)

        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)

        cm = confusion_matrix(y_test, y_pred)

        results.append([name, accuracy, precision, recall, f1, time_taken, cm])

    df = pd.DataFrame(results, columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F-1 Score', 'Time', 'Confusion Matrix'])

    return df

# Use the function
results_df = fit_ml_models(X_train_res, y_train_res, X_test, y_test)

results_df
#menggunakan data smote

import time

# Daftar model # tidak menggunakand ata smote
models = [
    ('Logistic Regression', LogisticRegression()),
    ('Decision Tree', DecisionTreeClassifier()),
    ('Random Forest', RandomForestClassifier()),
    ('Gradient Boosting', GradientBoostingClassifier()),
    ('KNN', KNeighborsClassifier())
]

# Fungsi untuk menghitung metrik
def calculate_metrics(model, X_train, X_test, y_train, y_test):
    start_time = time.time()
    model.fit(X_train, y_train)
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)
    end_time = time.time()
    time_taken = end_time - start_time
    return {
        'Accuracy (train set)': round(accuracy_score(y_train, y_train_pred), 3),
        'Accuracy (test set)': round(accuracy_score(y_test, y_test_pred), 3),
        'Precision (train set)': round(precision_score(y_train, y_train_pred), 3),
        'Precision (test set)': round(precision_score(y_test, y_test_pred), 3),
        'Recall (train set)': round(recall_score(y_train, y_train_pred), 3),
        'Recall (test set)': round(recall_score(y_test, y_test_pred), 3),
        'F-1 Score (train set)': round(f1_score(y_train, y_train_pred), 3),
        'F-1 Score (test set)': round(f1_score(y_test, y_test_pred), 3),
        'AUC (train set)': round(roc_auc_score(y_train, y_train_pred), 3),
        'AUC (test set)': round(roc_auc_score(y_test, y_test_pred), 3),
        'Time taken': round(time_taken, 3)
    }

# DataFrame untuk menyimpan hasil
results = []

# Iterasi melalui setiap model
for name, model in models:
    result = calculate_metrics(model, X_train, X_test, y_train, y_test)
    result['Model'] = name
    results.append(result)

    # Plot confusion matrix
    cm = confusion_matrix(y_test, model.predict(X_test))
    plt.figure(figsize=(5,5))
    sns.heatmap(cm, annot=True, fmt=".0f", linewidths=.5, square = True, cmap = 'Blues_r');
    plt.ylabel('Actual label');
    plt.xlabel('Predicted label');
    plt.title(name, size = 15);
    plt.show()

# Menampilkan hasil
df_results = pd.DataFrame(results)
df_results.set_index('Model', inplace=True)
df_results

"""# memilih model terbaik

Berdasarkan metrik-metrik di atas, secara umum, Random Forest tampaknya memberikan hasil yang baik dengan Precision yang baik, tingginya Recall, dan F1 Score yang seimbang.
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
import time

# Create the model
rf = RandomForestClassifier(random_state=42)

# Record the start time
start_time = time.time()

# Train the model
rf.fit(X_train_res, y_train_res)

# Record the end time
end_time = time.time()

# Make predictions
y_pred = rf.predict(X_test)

# Calculate metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Create a DataFrame for the results
results_rf = pd.DataFrame({
    'mechin_model': ['Random Forest'],
    'Accuracy': [accuracy],
    'Precision': [precision],
    'Recall': [recall],
    'F-1 Score': [f1],
    'Time taken': [end_time - start_time]
})

results_rf